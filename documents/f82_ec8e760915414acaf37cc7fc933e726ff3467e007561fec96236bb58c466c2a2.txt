
 
  
   
  Question answering - Wikipedia 
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
  
 
   
   
    
   
    
    
    
    
   Question answering 
    
    
     From Wikipedia, the free encyclopedia
     
     
     
     Jump to navigation Jump to search 
    
     
      
      
       For other uses, see Question and Answer.
       
      
       Computer science discipline
       
      Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language.  
      
       
       
        Contents
        
        
        1 Overview 
        2 History 
        3 Architecture 
        4 Question answering methods 
          
          4.1 Open domain question answering 
          4.2 Mathematical question answering 
           
        5 Progress 
        6 References 
        7 Further reading 
        8 External links 
        
       
      Overview 
      A question answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question answering systems can pull answers from an unstructured collection of natural language documents. 
      Some examples of natural language document collections used for question answering systems include:  
      
       a local collection of reference texts 
       internal organization documents and web pages 
       compiled newswire reports 
       a set of Wikipedia pages 
       a subset of World Wide Web pages
       
      Question answering research attempts to deal with a wide range of question types including: fact, list, definition, How, Why, hypothetical, semantically constrained, and cross-lingual questions.  
      
       Closed-domain question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can exploit domain-specific knowledge frequently formalized in ontologies. Alternatively, closed-domain might refer to a situation where only a limited type of questions are accepted, such as questions asking for descriptive rather than procedural information. Question answering systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimer's disease. 
       Open-domain question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.
       
      History 
      Two early question answering systems were BASEBALL and LUNAR. BASEBALL answered questions about Major League Baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both question answering systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain question answering systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR, the first chatterbot programs. 
      SHRDLU was a highly successful question-answering program developed by Terry Winograd in the late 1960s and early 1970s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility of asking the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program. 
      In the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The question answering systems developed to interface with these expert systems produced more repeatable and valid responses to questions within an area of knowledge. These expert systems closely resembled modern question answering systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern question answering systems rely on statistical processing of a large, unstructured, natural language text corpus. 
      The 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning. 
      Specialized natural language question answering systems have been developed, such as EAGLi for health and life scientists.  
      Architecture 
      As of 2001, question answering systems typically included a question classifier module that determines the type of question and the type of answer.  
      Question answering methods 
      Question answering is very dependent on a good search corpus—for without documents containing the answer, there is little any question answering system can do. It thus makes sense that larger collection sizes generally lend well to better question answering performance, unless the question domain is orthogonal to the collection. The notion of data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents, leading to two benefits:  
      
       By having the right information appear in many forms, the burden on the question answering system to perform complex NLP techniques to understand the text is lessened. 
       Correct answers can be filtered from false positives by relying on the correct answer to appear more times in the documents than instances of incorrect ones.
       
      Some question answering systems rely heavily on automated reasoning.  
      Open domain question answering 
      
       
        
         
          
           
          
         
          
           This section needs additional citations for verification. Please help improve this article by adding citations to reliable sources. Unsourced material may be challenged and removed. (January 2016) (Learn how and when to remove this template message)
          
        
       
       
      In information retrieval, an open domain question answering system aims at returning an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from computational linguistics, information retrieval and knowledge representation for finding answers. 
      The system takes a natural language question as an input rather than a set of keywords, for example, "When is the national day of China?" The sentence is then transformed into a query through its logical form. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type. 
      Keyword extraction is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly, i.e., "Who", "Where" or "How many", these words tell the system that the answers should be of type "Person", "Location", or "Number", respectively. In the example above, the word "When" indicates that the answer should be of type "Date". POS (part-of-speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is "Chinese National Day", the predicate is "is" and the adverbial modifier is "when", therefore the answer type is "Date". Unfortunately, some interrogative words like "Which", "What" or "How" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as WordNet can then be used for understanding the context. 
      Once the question type has been identified, an information retrieval system is used to find a set of documents containing the correct keywords. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as "Who" or "Where", a named-entity recogniser is used to find relevant "Person" and "Location" names from the retrieved documents. Only the relevant paragraphs are selected for ranking. 
      A vector space model can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. An inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is "1st Oct."  
      Mathematical question answering 
      An open source math-aware question answering system based on Ask Platypus and Wikidata was published in 2018. The system takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as succinct answer. The resulting formula is translated into a computable form, allowing the user to insert values for the variables. Names and values of variables and common constants are retrieved from Wikidata if available. It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set. 
      MathQA methods need to combine natural and formula language. One possible approach is to perform supervised annotation via Entity Linking. The "ARQMath Task" at CLEF 2020 was launched to address the problem of linking newly posted questions from the platform Math Stack Exchange (MSE) to existing ones that were already answered by the community. The lab was motivated by the fact that Mansouri et al. discovered that 20% of the mathematical queries in general-purpose search engines are expressed as well-formed questions. It contained two separate sub-tasks. Task 1: "Answer retrieval" matching old post answers to newly posed questions and Task 2: "Formula retrieval" matching old post formulae to new questions. Starting with the domain of mathematics, which involves formula language, the goal is to later extend the task to other domains (e.g., STEM disciplines, such as chemistry, biology, etc.), which employ other types of special notation (e.g., chemical formulae).  
      Progress 
      Question answering systems have been extended in recent years to encompass additional domains of knowledge For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current question answering research topics include:  
      
       interactivity—clarification of questions or answers 
       answer reuse or caching 
       semantic parsing 
       answer presentation 
       knowledge representation and semantic entailment 
       social media analysis with question answering systems 
       sentiment analysis 
       utilization of thematic roles 
       semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts 
       utilization of linguistic resources, such as WordNet, FrameNet, and the similar 
       Image captioning for visual question answering 
       Embodied question answering
       
      In 2011, Watson, a question answering computer system developed by IBM, competed in two exhibition matches of Jeopardy! against Brad Rutter and Ken Jennings, winning by a significant margin. Facebook Research has made their DrQA system available under an open source license. This system has been used for open domain question answering using Wikipedia as knowledge source.  
      References 
      
       
        
        ^ Philipp Cimiano; Christina Unger; John McCrae (1 March 2014). Ontology-Based Interpretation of Natural Language. Morgan &amp; Claypool Publishers. ISBN&nbsp;978-1-60845-990-2.  
        ^ Roser Morante, Martin Krallinger, Alfonso Valencia and Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer's Disease. CLEF 2012 Evaluation Labs and Workshop. September 17, 2012  
        ^ 
          GREEN JR, Bert F; et&nbsp;al. (1961). "Baseball: an automatic question-answerer" (PDF). Western Joint IRE-AIEE-ACM Computer Conference: 219–224.  
        ^ 
          Woods, William A; Kaplan, R. (1977). "Lunar rocks in natural English: Explorations in natural language question answering". Linguistic Structures Processing 5. 5: 521–569.  
        ^ 
          "EAGLi platform - Question Answering in MEDLINE". candy.hesge.ch. Retrieved 2021-12-02.  
        ^ Hirschman, L. &amp; Gaizauskas, R. (2001) Natural Language Question Answering. The View from Here. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.  
        ^ Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).  
        ^ Moldovan, Dan, et al. "Cogex: A logic prover for question answering." Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1. Association for Computational Linguistics, 2003.  
        ^ Furbach, Ulrich, Ingo Glöckner, and Björn Pelzer. "An application of automated reasoning in natural language question answering." Ai Communications 23.2-3 (2010): 241-265.  
        ^ 
          Sun, Haitian; Dhingra, Bhuwan; Zaheer, Manzil; Mazaitis, Kathryn; Salakhutdinov, Ruslan; Cohen, William (2018). "Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text". Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Brussels, Belgium. pp.&nbsp;4231–4242. arXiv:1809.00782. doi:10.18653/v1/D18-1455. S2CID&nbsp;52154304.  
        ^ 
          Harabagiu, Sanda; Hickl, Andrew (2006). "Methods for using textual entailment in open-domain question answering". Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL '06. pp.&nbsp;905–912. doi:10.3115/1220175.1220289.  
        ^ 
          Moritz Schubotz; Philipp Scharpf; et&nbsp;al. (12 September 2018). "Introducing MathQA: a Math-Aware question answering system". Information Discovery and Delivery. Emerald Publishing Limited. 46 (4): 214–224. arXiv:1907.01642. doi:10.1108/IDD-06-2018-0022.  
        ^ a b 
          Zanibbi, Richard; Oard, Douglas W.; Agarwal, Anurag; Mansouri, Behrooz (2020), "Overview of ARQMath 2020: CLEF Lab on Answer Retrieval for Questions on Math", Lecture Notes in Computer Science, Cham: Springer International Publishing, pp.&nbsp;169–193, doi:10.1007/978-3-030-58219-7_15, ISBN&nbsp;978-3-030-58218-0, retrieved 2021-06-09  
        ^ a b 
          Bela, Scharpf, Philipp Schubotz, Moritz Greiner-Petter, Andre Ostendorff, Malte Teschke, Olaf Gipp (2020-12-04). ARQMath Lab: An Incubator for Semantic Formula Search in zbMATH Open?. OCLC&nbsp;1228449497.  
        ^ 
          Mansouri, Behrooz; Zanibbi, Richard; Oard, Douglas W. (June 2019). "Characterizing Searches for Mathematical Concepts". 2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL). IEEE: 57–66. doi:10.1109/jcdl.2019.00019. ISBN&nbsp;978-1-7281-1547-4. S2CID&nbsp;198972305.  
        ^ 
          Paşca, Marius (2005). "Book Review New Directions in Question Answering Mark T. Maybury (editor) (MITRE Corporation) Menlo Park, CA: AAAI Press and Cambridge, MA: The MIT Press, 2004, xi+336 pp; paperbound, ISBN 0-262-63304-3, $40.00, £25.95". Computational Linguistics. 31 (3): 413–417. doi:10.1162/089120105774321055. S2CID&nbsp;12705839.  
        ^ a b Anderson, Peter, et al. "Bottom-up and top-down attention for image captioning and visual question answering." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.  
        ^ Zhu, Linchao, et al. "Uncovering the temporal context for video question answering." International Journal of Computer Vision 124.3 (2017): 409-421.  
        ^ Quarteroni, Silvia, and Suresh Manandhar. "Designing an interactive open-domain question answering system." Natural Language Engineering 15.1 (2009): 73-95.  
        ^ Light, Marc, et al. "Reuse in Question Answering: A Preliminary Study." New Directions in Question Answering. 2003.  
        ^ Yih, Wen-tau, Xiaodong He, and Christopher Meek. "Semantic parsing for single-relation question answering." Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2014.  
        ^ Perera, R., Nand, P. and Naeem, A. 2017. Utilizing typed dependency subtree patterns for answer sentence generation in question answering systems.  
        ^ de Salvo Braz, Rodrigo, et al. "An inference model for semantic entailment in natural language." Machine Learning Challenges Workshop. Springer, Berlin, Heidelberg, 2005.  
        ^ 
          "BitCrawl by Hobson Lane". Archived from the original on October 27, 2012. Retrieved 2012-05-29.}: CS1 maint: bot: original URL status unknown (link)  
        ^ Perera, R. and Perera, U. 2012. Towards a thematic role based target identification model for question answering.  
        ^ 
          Bahadorreza Ofoghi; John Yearwood &amp; Liping Ma (2008). The impact of semantic class identification and semantic role labeling on natural language answer extraction. The 30th European Conference on Information Retrieval (ECIR'08). Springer Berlin Heidelberg. pp.&nbsp;430–437. doi:10.1007/978-3-540-78646-7_40.  
        ^ 
          Bahadorreza Ofoghi; John Yearwood &amp; Liping Ma (2009). "The impact of frame semantic annotation levels, frame‐alignment techniques, and fusion methods on factoid answer processing". Journal of the American Society for Information Science and Technology. 60 (2): 247–263. doi:10.1002/asi.20989.  
        ^ Das, Abhishek, et al. "Embodied question answering." Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.  
        ^ 
          Markoff, John (2011-02-16). "On 'Jeopardy!' Watson Win is All but Trivial". The New York Times.  
        ^ 
          "DrQA".  
        ^ 
          Chen, Danqi; Fisch, Adam; Weston, Jason; Bordes, Antoine (2017). "Reading Wikipedia to Answer Open-Domain Questions". arXiv:1704.00051 .  
       
       
      Further reading 
      
       Dragomir R. Radev, John Prager, and Valerie Samn. Ranking suspected answers to natural language questions using predictive annotation. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000. 
       John Prager, Eric Brown, Anni Coden, and Dragomir Radev. Question-answering by predictive annotation. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000. 
       
        Hutchins, W. John; Harold L. Somers (1992). An Introduction to Machine Translation. London: Academic Press. ISBN&nbsp;978-0-12-362830-5. 
       L. Fortnow, Steve Homer (2002/2003). A Short History of Computational Complexity. In D. van Dalen, J. Dawson, and A. Kanamori, editors, The History of Mathematical Logic. North-Holland, Amsterdam.
       
      External links 
      
       Question Answering Evaluation at NTCIR 
       Question Answering Evaluation at TREC 
       Question Answering Evaluation at CLEF 
       Quiz Question Answers 
       Online Question Answering System
       
      
       
      
      
       
        
         
          
           
            
             v
             t
             e
            
           
           
            Computable knowledge
           
         
         
          Topics andconcepts
          
            
            
             Alphabet of human thought 
             Authority control 
             Automated reasoning 
             Commonsense knowledge 
             Commonsense reasoning 
             Computability 
             Discovery system 
             Formal system 
             Inference engine 
             Knowledge base 
              
               Personal knowledge base
               
             Knowledge-based systems 
             Knowledge engineering 
             Knowledge extraction 
             Knowledge graph 
             Knowledge representation 
             Knowledge retrieval 
             Library classification 
             Logic programming 
             Ontology 
             Question answering 
             Semantic reasoner
             
           
         
         
          Proposals andimplementations
          
            
            
             Zairja 
             Ars Magna&nbsp;(1300) 
             An Essay towards a Real Character, and a Philosophical Language&nbsp;(1688) 
             Calculus ratiocinator and characteristica universalis&nbsp;(1700) 
             Dewey Decimal Classification&nbsp;(1876) 
             Begriffsschrift&nbsp;(1879) 
             Mundaneum&nbsp;(1910) 
             Logical atomism&nbsp;(1918) 
             Tractatus Logico-Philosophicus&nbsp;(1921) 
             Hilbert's program&nbsp;(1920s) 
             Incompleteness theorem&nbsp;(1931) 
             World Brain&nbsp;(1938) 
             Memex&nbsp;(1945) 
             General Problem Solver&nbsp;(1959) 
             Prolog&nbsp;(1972) 
             Cyc&nbsp;(1984) 
             Semantic Web&nbsp;(2001) 
             Wikipedia&nbsp;(2001) 
             Evi&nbsp;(2007) 
             Wolfram Alpha&nbsp;(2009) 
             Watson&nbsp;(2011) 
             Siri&nbsp;(2011) 
             Google Knowledge Graph&nbsp;(2012) 
             Wikidata&nbsp;(2012) 
             Cortana&nbsp;(2014) 
             Viv&nbsp;(2016)
             
           
         
         
          In fiction
          
            
            
             The Engine (Gulliver's Travels, 1726) 
             Joe ("A Logic Named Joe", 1946) 
             The Librarian (Snow Crash, 1992) 
             Dr. Know (A.I. (film), 2001) 
             Waterhouse (The Baroque Cycle, 2003)
             
            See also: Logic machines in fiction and List of fictional computers  
           
         
        
       
       
      
       
      
      
       
        
         
          
           
           
            
             v
             t
             e
            
           
           
            Natural language processing
           
         
         
          General terms
          
            
            
             AI-complete 
             Bag-of-words 
             n-gram 
              
               Bigram 
               Trigram
               
             Computational linguistics 
             Natural-language understanding 
             Stop words 
             Text processing
             
           
         
         
          Text analysis
          
            
            
             Collocation extraction 
             Concept mining 
             Coreference resolution 
             Deep linguistic processing 
             Distant reading 
             Information extraction 
             Named-entity recognition 
             Ontology learning 
             Parsing 
             Part-of-speech tagging 
             Semantic role labeling 
             Semantic similarity 
             Sentiment analysis 
             Terminology extraction 
             Text mining 
             Textual entailment 
             Truecasing 
             Word-sense disambiguation 
             Word-sense induction
             
           
           
            
             
              Text segmentation
              
                
                
                 Compound-term processing 
                 Lemmatisation 
                 Lexical analysis 
                 Text chunking 
                 Stemming 
                 Sentence segmentation 
                 Word segmentation
                 
               
             
            
           
            
           
         
         
          Automatic summarization
          
            
            
             Multi-document summarization 
             Sentence extraction 
             Text simplification
             
           
         
         
          Machine translation
          
            
            
             Computer-assisted 
             Example-based 
             Rule-based 
             Statistical 
             Transfer-based 
             Neural
             
           
         
         
          Distributional semantics models
          
            
            
             BERT 
             Document-term matrix 
             Explicit semantic analysis 
             fastText 
             GloVe 
             Latent semantic analysis 
             Word embedding 
             Word2vec
             
           
         
         
          Language resources, datasets and corpora
          
           
           
            
             
              Types and standards
              
                
                
                 Corpus linguistics 
                 Lexical resource 
                 Linguistic Linked Open Data 
                 Machine-readable dictionary 
                 Parallel text 
                 PropBank 
                 Semantic network 
                 Simple Knowledge Organization System 
                 Speech corpus 
                 Text corpus 
                 Thesaurus (information retrieval) 
                 Treebank 
                 Universal Dependencies
                 
               
             
             
              Data
              
                
                
                 BabelNet 
                 Bank of English 
                 DBpedia 
                 FrameNet 
                 Google Ngram Viewer 
                 ThoughtTreasure 
                 UBY 
                 WordNet
                 
               
             
            
           
           
         
         
          Automatic identificationand data capture
          
            
            
             Speech recognition 
             Speech segmentation 
             Speech synthesis 
             Natural language generation 
             Optical character recognition
             
           
         
         
          Topic model
          
            
            
             Document classification 
             Latent Dirichlet allocation 
             Pachinko allocation
             
           
         
         
          Computer-assistedreviewing
          
            
            
             Automated essay scoring 
             Concordancer 
             Grammar checker 
             Predictive text 
             Spell checker 
             Syntax guessing
             
           
         
         
          Natural languageuser interface
          
            
            
             Chatbot 
             Interactive fiction 
             Question answering 
             Virtual assistant 
             Voice user interface
             
           
         
         
          Other software
          
            
            
             Natural Language Toolkit 
             spaCy
             
           
         
        
       
          
     
     
      
      
     
      Retrieved from "https://en.wikipedia.org/w/index.php?title=Question_answering&amp;oldid=1064798316"
     
     
    
     
      Categories: 
      
       Artificial intelligence applications
       Natural language processing
       Computational linguistics
       Information retrieval genres
       Tasks of natural language processing
       Deep learning
      
     
     
      Hidden categories: 
      
       CS1 maint: bot: original URL status unknown
       Articles with short description
       Short description is different from Wikidata
       Articles needing additional references from January 2016
       All articles needing additional references
      
     
     
    
   
   
    
   
   
   Navigation menu 
    
     
      Personal tools  
      
      
       Not logged in
       Talk
       Contributions
       Create account
       Log in
       
      
     
     
      
       Namespaces  
       
       
        Article
        Talk
        
       
      
      
       
       English expanded collapsed  
       
        
       
      
     
     
      
       Views  
       
       
        Read
        Edit
        View history
        
       
      
      
       
       More expanded collapsed  
       
        
       
      
      
       
        Search  
        
         
          
          
          
          
         
        
       
      
     
    
    
      
     
     
      Navigation  
      
      
       Main page
       Contents
       Current events
       Random article
       About Wikipedia
       Contact us
       Donate
       
      
     
     
      Contribute  
      
      
       Help
       Learn to edit
       Community portal
       Recent changes
       Upload file
       
      
     
     
      Tools  
      
      
       What links here
       Related changes
       Upload file
       Special pages
       Permanent link
       Page information
       Cite this page
       Wikidata item
       
      
     
     
      Print/export  
      
      
       Download as PDF
       Printable version
       
      
     
     
      Languages  
      
      
       Български
       Català
       Español
       Euskara
       فارسی
       Français
       한국어
       Hrvatski
       Italiano
       日本語
       Polski
       Română
       Русский
       Српски / srpski
       Українська
       中文
       
      
       Edit links
       
      
     
    
   
   
    
     This page was last edited on 10 January 2022, at 07:23&nbsp;(UTC). 
    Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. 
    
    
    Privacy policy 
    About Wikipedia 
    Disclaimers 
    Contact Wikipedia 
    Mobile view 
    Developers 
    Statistics 
    Cookie statement 
    
    
     
     
    
   
   
   
    
 
