
 
  
   
  Relevance feedback - Wikipedia 
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
  
 
   
   
    
   
    
    
    
    
   Relevance feedback 
    
    
     From Wikipedia, the free encyclopedia
     
     
     
     Jump to navigation Jump to search 
    
     
      Relevance feedback is a feature of some information retrieval systems. The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query. We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.  
      
       
       
        Contents
        
        
        1 Explicit feedback 
        2 Implicit feedback 
        3 Blind feedback 
        4 Using relevance information 
        5 References 
        6 Further reading 
        
       
      Explicit feedback 
      Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as relevance judgments. 
      Users may indicate relevance explicitly using a binary or graded relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance. An example of this would be the SearchWiki feature implemented by Google on their search website. 
      The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known Rocchio algorithm. 
      A performance metric which became popular around 2005 to measure the usefulness of a ranking algorithm based on the explicit relevance feedback is NDCG. Other measures include precision at k and mean average precision.  
      Implicit feedback 
      Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response. 
      The key differences of implicit relevance feedback from that of explicit include:  
      
       the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and 
       the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback
       
      An example of this is dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.  
      Blind feedback 
      Pseudo relevance feedback, also known as blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:  
      
       Take the results returned by initial query as relevant results (only top k with k being between 10 and 50 in most experiments). 
       Select top 20-30 (indicative number) terms from these documents using for instance tf-idf weights. 
       Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.
       
      Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments. 
      This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis. Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents. Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic. 
      Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.  
      Using relevance information 
      Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query. Relevance feedback is often implemented using the Rocchio algorithm.  
      References 
      
       
        
        ^ "Archived copy" (PDF). www.scils.rutgers.edu. Archived from the original (PDF) on 16 March 2004. Retrieved 12 January 2022.}: CS1 maint: archived copy as title (link)  
        ^ Jansen, B. J. and McNeese, M. D. 2005. Evaluating the effectiveness of and patterns of interactions with automated assistance in IR systems. Journal of the American Society for Information Science and Technology. 56(14), 1480-1503  
        ^ Kelly, Diane, and Jaime Teevan. "Implicit feedback for inferring user preference: a bibliography." ACM SIGIR Forum. Vol. 37. No. 2. ACM, 2003.  
        ^ 
          "Archived copy" (PDF). haystack.lcs.mit.edu. Archived from the original (PDF) on 11 June 2007. Retrieved 12 January 2022.}: CS1 maint: archived copy as title (link)  
        ^ Jinxi Xu and W. Bruce Croft, Query expansion using local and global document analysis, in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.  
        ^ Yuanhua Lv and ChengXiang Zhai, Positional relevance model for pseudo-relevance feedback, in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.  
       
       
      Further reading 
      
       Relevance feedback lecture notes - Jimmy Lin's lecture notes, adapted from Doug Oard's 
        - chapter from Modern Information Retrieval 
       Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval: Implementing and Evaluating Search Engines. MIT Press, Cambridge, Mass., 2010.
          
     
     
      
      
     
      Retrieved from "https://en.wikipedia.org/w/index.php?title=Relevance_feedback&amp;oldid=1065261966"
     
     
    
     
      Categories: 
      
       Internet search algorithms
       Information retrieval evaluation
      
     
     
      Hidden categories: 
      
       CS1 maint: archived copy as title
       All articles with unsourced statements
       Articles with unsourced statements from March 2011
      
     
     
    
   
   
    
   
   
   Navigation menu 
    
     
      Personal tools  
      
      
       Not logged in
       Talk
       Contributions
       Create account
       Log in
       
      
     
     
      
       Namespaces  
       
       
        Article
        Talk
        
       
      
      
       
       Variants expanded collapsed  
       
        
       
      
     
     
      
       Views  
       
       
        Read
        Edit
        View history
        
       
      
      
       
       More expanded collapsed  
       
        
       
      
      
       
        Search  
        
         
          
          
          
          
         
        
       
      
     
    
    
      
     
     
      Navigation  
      
      
       Main page
       Contents
       Current events
       Random article
       About Wikipedia
       Contact us
       Donate
       
      
     
     
      Contribute  
      
      
       Help
       Learn to edit
       Community portal
       Recent changes
       Upload file
       
      
     
     
      Tools  
      
      
       What links here
       Related changes
       Upload file
       Special pages
       Permanent link
       Page information
       Cite this page
       Wikidata item
       
      
     
     
      Print/export  
      
      
       Download as PDF
       Printable version
       
      
     
     
      Languages  
      
      
       Deutsch
       Français
       中文
       
      
       Edit links
       
      
     
    
   
   
    
     This page was last edited on 12 January 2022, at 17:00&nbsp;(UTC). 
    Text is available under the Creative Commons Attribution-ShareAlike License; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization. 
    
    
    Privacy policy 
    About Wikipedia 
    Disclaimers 
    Contact Wikipedia 
    Mobile view 
    Developers 
    Statistics 
    Cookie statement 
    
    
     
     
    
   
   
   
    
 
